{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text4(text):\n",
    "    new_text = []\n",
    "    text = text.lower()\n",
    "\n",
    "    #remove punctuation\n",
    "    text = text.replace(',', ' ')\n",
    "    text = re.sub(r'[{}]'.format(string.punctuation), ' ', text)\n",
    "    text = ''.join('' if a in string.punctuation else a for a in text)\n",
    "    \n",
    "    #a_dum\n",
    "    text = text.split()\n",
    "    for i,word in enumerate(text):\n",
    "        if 'dok' in word and word.startswith('dok') and len(word) > 3:\n",
    "            text[i] = 'dok ' + word[3:]\n",
    "    text = \" \".join(text)\n",
    "    #c_dum\n",
    "    \n",
    "    #normalization into standar word \n",
    "    for text in text.split():\n",
    "        if text not in std_word_replace:\n",
    "            new_text.append(text)\n",
    "        elif text in std_word_replace:\n",
    "            new_text += std_word_replace[text].split()\n",
    "    #stopword\n",
    "    new_text = ' '.join(stemming.stem(text) for text in new_text if text not in fin_stp)\n",
    "    #d_dum  \n",
    "\n",
    "    # #create token \n",
    "    new_text = tokenize(new_text)\n",
    "    new_text = list(new_text)\n",
    "\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    #remove punctuation\n",
    "    text = text.replace(',', ' ')\n",
    "    text = re.sub(r'[{}]'.format(string.punctuation), ' ', text)\n",
    "    removed_punct = ''.join('' if a in string.punctuation else a for a in text)\n",
    "\n",
    "    return removed_punct\n",
    "\n",
    "def dok_word(removed_punct):\n",
    "    \n",
    "    split_dok = removed_punct.split()\n",
    "    for i, word in enumerate(split_dok):\n",
    "        if 'dok' in word and word.startswith('dok') and len(word) > 3:\n",
    "            split_dok[i] = 'dok ' + word[3:]\n",
    "    removed_dok = \" \".join(split_dok)\n",
    "    return removed_dok\n",
    "\n",
    "def normalization(removed_dok):\n",
    "    temp_list = []\n",
    "    temp_normalization = removed_dok.split()\n",
    "    for i in temp_normalization:\n",
    "        if i not in std_word_replace:\n",
    "            temp_list.append(temp_normalization)\n",
    "        elif i in std_word_replace:\n",
    "            temp_list += std_word_replace[temp_normalization].split()\n",
    "\n",
    "    return temp_list\n",
    "\n",
    "def stopword(temp_list):\n",
    "    clean_text = ' '.join(stemming.stem(text) for text in temp_list if text not in fin_stop)\n",
    "    return clean_text\n",
    "\n",
    "def tokenize(clean_text):\n",
    "    token = tokenize(clean_text)\n",
    "    token_text = list(token)\n",
    "    return token_text\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def clean_text4(text):\n",
    "    new_text = []\n",
    "    text = text.lower()\n",
    "\n",
    "    #remove punctuation\n",
    "    text = text.replace(',', ' ')\n",
    "    text = re.sub(r'[{}]'.format(string.punctuation), ' ', text)\n",
    "    text = ''.join('' if a in string.punctuation else a for a in text)\n",
    "    \n",
    "    #a_dum\n",
    "    text = text.split()\n",
    "    for i,word in enumerate(text):\n",
    "        if 'dok' in word and word.startswith('dok') and len(word) > 3:\n",
    "            text[i] = 'dok ' + word[3:]\n",
    "    text = \" \".join(text)\n",
    "    #c_dum\n",
    "    \n",
    "    #normalization into standar word \n",
    "    for text in text.split():\n",
    "        if text not in std_word_replace:\n",
    "            new_text.append(text)\n",
    "        elif text in std_word_replace:\n",
    "            new_text += std_word_replace[text].split()\n",
    "    #stopword\n",
    "    new_text = ' '.join(stemming.stem(text) for text in new_text if text not in fin_stp)\n",
    "    #d_dum  \n",
    "\n",
    "    # #create token \n",
    "    new_text = tokenize(new_text)\n",
    "    new_text = list(new_text)\n",
    "\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    #remove punctuation\n",
    "    text = text.replace(',', ' ')\n",
    "    text = re.sub(r'[{}]'.format(string.punctuation + '\\n'), ' ', text)\n",
    "    text = ''.join('' if a in string.punctuation else a for a in text)\n",
    "    return text\n",
    "\n",
    "def dok_word(text):\n",
    "    text = remove_punct(text)\n",
    "    split_dok = text.split()\n",
    "    for i, word in enumerate(split_dok):\n",
    "        if 'dok' in word and word.startswith('dok') and len(word) > 3:\n",
    "            split_dok[i] = 'dok ' + word[3:]\n",
    "    text = \" \".join(split_dok)\n",
    "    return text \n",
    "\n",
    "def normalization(text):\n",
    "    text = dok_word(text)\n",
    "    temp_list = []\n",
    "    temp_normalization = text.split()\n",
    "    for i in temp_normalization:\n",
    "        if i not in std_word_replace:\n",
    "            temp_list.append(i)\n",
    "        elif i in std_word_replace:\n",
    "            temp_list += std_word_replace[i].split()\n",
    "    temp_list = ' '.join(stemming.stem(text) for text in temp_list if text not in fin_stp)\n",
    "    return temp_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def clean_text2(text):\n",
    "    new_text = []\n",
    "    text = text.lower() # Lowercase\n",
    "    # Remove punctuations\n",
    "    text = ''.join(' ' if a in string.punctuation else a for a in text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Loop each word in a sentence\n",
    "    for kata in text.split(): \n",
    "        # Keep word not in slang or standard word\n",
    "        if kata not in std_word_replace: \n",
    "            new_text.append(kata) \n",
    "        # Replace non-formal word with standard word\n",
    "        elif kata in std_word_replace:\n",
    "            new_text+=std_word_replace[kata].split() \n",
    "    # Join words without stopwords after stemming\n",
    "    new_text = ' '.join(\n",
    "        stemmer.stem(word) for word in new_text if word not in final_stopword #perubahan hanya ada di final_stopword\n",
    "    )\n",
    "    return new_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def stopword(temp_list):\n",
    "    clean_text = ' '.join(stemming.stem(word) for word in temp_list if word not in fin_stp)\n",
    "    return clean_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def normalization(removed_dok):\n",
    "    temp_list = []\n",
    "    temp_normalization = removed_dok.split()\n",
    "    for i in temp_normalization:\n",
    "        if i not in std_word_replace:\n",
    "            temp_list.append(i)\n",
    "        elif i in std_word_replace:\n",
    "            temp_list += std_word_replace[i].split()\n",
    "    temp_list = ' '.join(stemming.stem(text) for text in temp_list if text not in fin_stp)\n",
    "    return temp_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def clean_text2(text):\n",
    "    new_text = []\n",
    "    text = text.lower()\n",
    "\n",
    "    #remove punctuation\n",
    "    text = ''.join('' if a in string.punctuation else a for a in text)\n",
    "    #a_dum\n",
    "    text = text.split()\n",
    "    for i,word in enumerate(text):\n",
    "        if 'dok' in word and word.startswith('dok') and len(word) > 3:\n",
    "            text[i] = 'dok ' + word[3:]\n",
    "    text = \" \".join(text)\n",
    "    #c_dum\n",
    "    \n",
    "    #normalization into standar word \n",
    "    for text in text.split():\n",
    "        if text not in std_word_replace:\n",
    "            new_text.append(text)\n",
    "        elif text in std_word_replace:\n",
    "            new_text += std_word_replace[text].split()\n",
    "    #stopword\n",
    "    new_text = ' '.join(text for text in new_text if text not in fin_stp)\n",
    "    #d_dum\n",
    "    return new_text  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n-gram"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#ngram \n",
    "big19 = gensim.models.Phrases(df_train19['clean'], min_count=10, threshold=5)\n",
    "bigmod = gensim.models.phrases.Phraser(big19)\n",
    "\n",
    "trig19 = gensim.models.Phrases(big19[df_train19['clean']], min_count=15, threshold=2.5)\n",
    "trigmod = gensim.models.phrases.Phraser(trig19)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def make_big(texts):\n",
    "    return [bigmod[doc] for doc in texts]\n",
    "\n",
    "def make_trig(texts):\n",
    "    return [trigmod[bigmod[doc]] for doc in texts]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
